{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc7df8a-0be1-4344-ae71-ad7aca549013",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T12:58:11.688289600Z",
     "start_time": "2024-06-16T12:57:44.358522200Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# When training this model, Python version 3.8.10 has been used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f8142d-a54a-4def-85f8-7e20653722d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T12:58:11.688289600Z",
     "start_time": "2024-06-16T12:57:44.358522200Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing the libraries: \n",
    "# The versions that were used for this model are annotated per library.\n",
    "\n",
    "import os\n",
    "import numpy as np              # Version used: 1.23.1\n",
    "import glob\n",
    "import monai                    # Version used: 1.3.0\n",
    "from monai.transforms import * \n",
    "import SimpleITK as sitk        # Version used: 2.3.1\n",
    "import torch                    # Version used: 1.13.1+cu116\n",
    "from tqdm import tqdm\n",
    "import wandb                    # Version used: 0.17.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6064334",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf2ba2f-84ec-4375-b5ff-1aa957c4b02e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T12:58:11.726184200Z",
     "start_time": "2024-06-16T12:58:11.709231600Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A function 'build_dicts' is made to obtain the filenames from the given dataset.\n",
    "def build_dicts(data_path='database', mode=\"training\"):\n",
    "\n",
    "    # Check if the mode is training or testing:\n",
    "    if mode not in [\"training\", \"testing\"]:\n",
    "        raise ValueError(f\"Please choose a mode in ['training', 'testing']. Current mode is {mode}.\")\n",
    "\n",
    "    # Finding the .nii.gz files in the dataset matching the folder and directory names with the variability of patientnumbers and frames. \n",
    "    paths_xray = glob.glob(os.path.join(data_path, mode, 'patient*', 'patient*_frame[0-9][0-9].nii.gz'))\n",
    "\n",
    "    dicts = []\n",
    "    # Iterate over each file path and extract the ground truth files from the scan file. \n",
    "    for scan_file in paths_xray:\n",
    "        extension_index = scan_file.index(\".\")\n",
    "        gt_file = scan_file[:extension_index] + \"_gt\" + scan_file[extension_index:]\n",
    "\n",
    "        directory = scan_file.split(os.sep)[:-1]\n",
    "        cfg_file = os.path.join(*directory, \"Info.cfg\")\n",
    "\n",
    "        # Open the info.cfg files to extraxt the disease information for completeness.\n",
    "        with open(cfg_file, \"r\") as f:\n",
    "            line = f.readlines()[2]\n",
    "            disease = line.split(\": \")[1]\n",
    "\n",
    "        # Make a dictionary with a scan file path, the ground truth file path, and the disease\n",
    "        if os.path.exists(gt_file):\n",
    "            dicts.append({\"scan_file\": scan_file, \n",
    "                          \"gt_file\": gt_file, \n",
    "                          \"class\": disease})\n",
    "    return dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68edbb42-5cae-4f9d-8dff-67d8e0b029fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A function 'split_train_val' is made to make a training and a validation set using Numpy.\n",
    "def split_train_val(train_list, ratio):\n",
    "    # Making a random index where 10 indices are chosen that are used for the validation set.\n",
    "        # These 10 indices are obtained by dividing the entire length of the trainlist (which is 200) by two.\n",
    "        # This is divided by two, since the set needs to be splitted on patient level and there are 200 scans (a diastoly scan and a systole) of 100 patients.\n",
    "        # Then a ratio can be defined to choose the size of the validation set.\n",
    "    val_idx = np.random.choice(len(train_list)//2,\n",
    "                               int(len(train_list) * ratio)//2, replace=False)\n",
    "    \n",
    "    train_dicts = []\n",
    "    val_dicts = []\n",
    "    # Iterated over all patients, 10 patients are extracted based on the val_idx that is computed.\n",
    "    for sample in train_list:\n",
    "        patient = int(sample['scan_file'][-17:-15])\n",
    "        \n",
    "        # The validation dictionairy is computed with 10 patients and 20 files.\n",
    "        if patient in val_idx:\n",
    "            val_dicts.append(sample)\n",
    "            \n",
    "        # The training dictionairy is computed with 90 patients and 180 files. \n",
    "        else:\n",
    "            train_dicts.append(sample)\n",
    "            \n",
    "    return train_dicts, val_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3a7158-c41b-4b29-bc99-7092248d5787",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T12:58:11.746128600Z",
     "start_time": "2024-06-16T12:58:11.725186800Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A function 'ReadFiles' is made to read out scan files from a specific dictionary using SimpleITK.\n",
    "class ReadFiles(monai.transforms.Transform):\n",
    "    def __call__(self, sample):\n",
    "        \n",
    "        # Read out the scan file without the ground truth\n",
    "        image = sitk.ReadImage(sample[\"scan_file\"])\n",
    "\n",
    "        # Read out the scan file with the ground truth\n",
    "        mask = sitk.ReadImage(sample[\"gt_file\"])\n",
    "\n",
    "        # Returning a dictionary contain all the relevant information\n",
    "        return {\"img\": image,                      # Object of the scan file without the segmenation\n",
    "                \"mask\": mask,                      # Object of the mask of the ground truth segmentation\n",
    "                \"img_size\": image.GetSize(),       # Size of each scan file\n",
    "                \"img_spacing\": image.GetSpacing(), # Spacing of the voxels of each scan file\n",
    "                \"class\": sample[\"class\"],          # The disease label of each scan file\n",
    "                \"scan_file\": sample[\"scan_file\"]   # Path to the scan file\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a5f9c4-5aae-4fd5-8cf8-a92e0035e7a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T12:58:11.710228900Z",
     "start_time": "2024-06-16T12:58:11.693275400Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A function 'resample_img' is made to set the spacing of the SITK image\n",
    "def resample_img(itk_image, out_spacing, is_label=False):\n",
    "    original_spacing = itk_image.GetSpacing()\n",
    "    original_size = itk_image.GetSize()\n",
    "\n",
    "    # Calculate the output size, after the image has been respaced\n",
    "    out_size = [\n",
    "        int(np.round(original_size[0] * (original_spacing[0] / out_spacing[0]))),\n",
    "        int(np.round(original_size[1] * (original_spacing[1] / out_spacing[1]))),\n",
    "        original_size[2]]# The original spacing is kept in the third axis, so also the third size\n",
    "\n",
    "    resample = sitk.ResampleImageFilter()\n",
    "    resample.SetOutputSpacing(out_spacing + [original_spacing[2]]) # The original spacing is kept in the third axis\n",
    "    resample.SetSize(out_size)\n",
    "\n",
    "    if is_label:\n",
    "        resample.SetInterpolator(sitk.sitkNearestNeighbor) # Use nearest neighbour for labels\n",
    "    else:\n",
    "        resample.SetInterpolator(sitk.sitkBSpline)\n",
    "\n",
    "    return resample.Execute(itk_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6633d59a-ea9d-4133-aafc-a57e58e83310",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T12:58:11.782029300Z",
     "start_time": "2024-06-16T12:58:11.739148600Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A function 'EqualizeSpacing' is made to apply the equalization of the spacings to the images and the masks.\n",
    "class EqualizeSpacing(monai.transforms.Transform):\n",
    "    def __call__(self, sample):\n",
    "        # The original size and spacings of the images and masks are stored in the sample dictionary.\n",
    "        # This is done so the image can be transformed back after inference\n",
    "        sample['org_size'] = sample['img_size']\n",
    "        sample['org_spacing'] = sample['img_spacing']\n",
    "        \n",
    "        # Here the x and y dimensions are equally spaced to 1.25 by 1.25. \n",
    "        image = resample_img(sample['img'], [1.25, 1.25], False)\n",
    "        sample['img'] = image\n",
    "        sample['img_size'] = image.GetSize()\n",
    "        sample['img_spacing'] = image.GetSpacing()\n",
    "\n",
    "        # Here the x and y directions are equally spaced to 1.25 by 1.25 similarly as the images above.\n",
    "        mask = resample_img(sample['mask'], [1.25, 1.25], True)\n",
    "        sample['mask'] = mask\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02baa37-977b-4758-b158-b219a143ca8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T12:58:11.783027400Z",
     "start_time": "2024-06-16T12:58:11.755105200Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A function 'LoadData' is made to convert the SimpleITK objects to NumPy arrays.\n",
    "class LoadData(monai.transforms.Transform):\n",
    "    def __call__(self, sample):\n",
    "        # Converting the SimpleITK objects to NumPy arrays for the images and the masks.\n",
    "        sample['img'] = sitk.GetArrayFromImage(sample['img'])\n",
    "        sample['mask'] = sitk.GetArrayFromImage(sample['mask'])\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59bb789-a162-4418-bafd-1d04951f8043",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T12:58:13.287489100Z",
     "start_time": "2024-06-16T12:58:11.771059700Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_path = 'database'\n",
    "# Here the initial training set is computed. \n",
    "train_dict_list = build_dicts(data_path, mode='training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b63509-5f7d-4b9f-b97b-cb7ee5400f7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Here the validation set is computed where it includes 10% (ratio=0.1) of the patients and the training set is updated, since the validation set gets extracted. \n",
    "train_dict_list, val_dict_list = split_train_val(train_dict_list, ratio=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f406495-189b-4ba6-a67c-fa47efa14028",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T12:58:13.330371Z",
     "start_time": "2024-06-16T12:58:13.281509Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data augmentation is applied to increase the regularization of the model. \n",
    "# Two separate composed transformer functions are made for the training set and the dataset using the MONAI library.\n",
    "\n",
    "train_transforms = monai.transforms.Compose([\n",
    "    ReadFiles(),        # Applies the 'ReadFiles' function\n",
    "    EqualizeSpacing(),  # Applies the 'EqualizeSpacing' function\n",
    "    LoadData(),         # Applies the 'LoadData' function  \n",
    "    EnsureChannelFirstd(channel_dim=\"no_channel\", keys=[\"img\", \"mask\"]), \n",
    "    ScaleIntensityd(keys=[\"img\"]), # Normalize the intensity\n",
    "    RandRotated(keys=[\"img\", \"mask\"], prob=0.75, range_x=(0.4, 0.4), mode=['bilinear', 'nearest']), # Random rotate data between 0.4 and 0.4 radians to represent other orientations of the heart in a realistic range.\n",
    "    RandGaussianNoised(keys=[\"img\"],prob=0.4, std=0.1) # Add random Gaussian noise\n",
    "])\n",
    "\n",
    "val_transforms = monai.transforms.Compose([\n",
    "    ReadFiles(),        # Applies the 'ReadFiles' function  \n",
    "    EqualizeSpacing(),  # Applies the 'EqualizeSpacing' function\n",
    "    LoadData(),         # Applies the 'LoadData' function\n",
    "    EnsureChannelFirstd(channel_dim=\"no_channel\", keys=[\"img\", \"mask\"]),\n",
    "    ScaleIntensityd(keys=[\"img\"])  # Normalize the intensity\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a510966c-4ca6-472b-87fe-74d79fd05e9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T12:59:44.794236600Z",
     "start_time": "2024-06-16T12:58:13.330371Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Applying the composed transform function to the train and validation data sets.\n",
    "train_dataset = monai.data.CacheDataset(data=train_dict_list, transform=train_transforms)\n",
    "val_dataset = monai.data.CacheDataset(data=val_dict_list, transform=val_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f69c88d",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Splitting the scans into slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886b6c3a-80f9-4d98-a1fc-51a225e2974c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T12:59:44.797226800Z",
     "start_time": "2024-06-16T12:59:44.778278700Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Here the axial slices are selected for the 2D model.\n",
    "patch_func = monai.data.PatchIterd(\n",
    "    keys=[\"img\", \"mask\"],\n",
    "    patch_size=(1, None, None),\n",
    "    start_pos=(0, 0, 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbc05cc-e0e4-4fb9-a535-562629ec4d50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T12:59:44.835120800Z",
     "start_time": "2024-06-16T12:59:44.787253800Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A function is made to define the conditions for a sliding window applied to the slices.\n",
    "patch_transform = monai.transforms.Compose([\n",
    "    SqueezeDimd(keys=['img', 'mask'], dim=0),\n",
    "    monai.transforms.RandSpatialCropd(keys=['img', 'mask'], roi_size=[176, 176], random_size=False) # Sliding window of 176 by 176 pixels.\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d9f1f5-dc24-415a-8f76-96aceb37cfb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T12:59:44.837115500Z",
     "start_time": "2024-06-16T12:59:44.799220600Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The patch_func and patch_transform are used to obtain the axial slices for the training and the validation set. \n",
    "train_patch_ds = monai.data.GridPatchDataset(\n",
    "    data=train_dataset, patch_iter=patch_func, transform=patch_transform,\n",
    "    with_coordinates=False)\n",
    "\n",
    "val_patch_ds = monai.data.GridPatchDataset(\n",
    "    data=val_dataset, patch_iter=patch_func, transform=patch_transform,\n",
    "    with_coordinates=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3b5c9f-6e3a-4509-92a9-9f1799e74a15",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Defining training and validation loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e31a486-0fa1-492d-8a38-a849f1f0a51d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T12:59:45.777653700Z",
     "start_time": "2024-06-16T12:59:44.810189600Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining the training and validation loaders. \n",
    "train_loader = monai.data.DataLoader(\n",
    "    train_patch_ds,\n",
    "    batch_size=32,\n",
    "    num_workers=0,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "val_loader = monai.data.DataLoader(\n",
    "    val_patch_ds,\n",
    "    batch_size=32,\n",
    "    num_workers=0,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a775dc85-c092-4c00-8cd9-7c4311ee1429",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T12:59:45.794606300Z",
     "start_time": "2024-06-16T12:59:45.778650900Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a152cf59",
   "metadata": {},
   "source": [
    "## Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff20b0c-cdd7-46e8-addb-456c7e266ff4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T12:59:46.477869100Z",
     "start_time": "2024-06-16T12:59:45.803581700Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A 2D U-net is created with the MONAI library\n",
    "model = monai.networks.nets.UNet(\n",
    "    spatial_dims=2,                   # Two dimensions (axial 2D slice)\n",
    "    in_channels=1,                    # In channel = scan-file\n",
    "    out_channels=4,                   # Output channel = background, right endocardium, left endocardium and left myocardium\n",
    "    channels=(48, 96, 192, 384, 768), # Defined layers\n",
    "    strides=(2, 2, 2, 2),\n",
    "    num_res_units=2,\n",
    "    dropout=0.25                      # Drop-out for regularization of the model\n",
    ").to(device)\n",
    "\n",
    "# Defining the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "# Defining the Dice loss function\n",
    "loss_function = monai.losses.DiceLoss(batch=True, softmax=True, to_onehot_y=True, include_background=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c95086",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ba1bf0-d2cc-4811-a8c7-49b5b0a104af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T16:12:18.294321200Z",
     "start_time": "2024-06-16T12:59:46.506789600Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Choosing amount of epochs\n",
    "epochs = 150\n",
    "\n",
    "# Initiate a training progress on weights and biases.\n",
    "run = wandb.init(\n",
    "    name='Unet_2d'\n",
    ")\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    model.train()\n",
    "\n",
    "    train_loss = 0\n",
    "    steps = 0\n",
    "    for train_batch in train_loader:\n",
    "        steps += 1\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(train_batch['img'].float().to(device))\n",
    "        loss = loss_function(outputs, train_batch['mask'].to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= steps\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    val_loss = 0\n",
    "    steps = 0\n",
    "    for val_batch in val_loader:\n",
    "        steps += 1\n",
    "        outputs = model(val_batch['img'].float().to(device))\n",
    "        loss = loss_function(outputs, val_batch['mask'].to(device))\n",
    "        val_loss += loss.item()\n",
    "    val_loss /= steps\n",
    "\n",
    "    wandb.log({'epoch': epoch, 'train_loss': train_loss, 'val_loss': val_loss})\n",
    "\n",
    "run.finish()\n",
    "torch.save(model.state_dict(), r'trainedUNet2D_noise04en0.1std.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
